





# Importing necesssary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Read the dataset
df_orgs = pd.read_csv("orgs_full.csv")
df_exs  = pd.read_csv("exercises_full.csv")

# Info and data types of the datasets & columns
df_orgs.info(), df_exs.info()


df_orgs.head()


df_exs.head()





# Checking missing values

print(df_orgs.isna().sum()) 
print()
print(df_exs.isna().sum())


'''
orgs_full.csv has multiple numerical valriables like Maturity, Complexity, ExcerciseFrequency.
Lets see the numeric distribution of these features, This can also determine how the recommender 
will perform based on distribution of data.
'''

org_num = ["Maturity", "Complexity", "ExerciseFrequency"]

fig, axes = plt.subplots(1, len(org_num), figsize=(18, 4))  # 1 row, 3 columns

for ax, c in zip(axes, org_num):
    sns.histplot(df_orgs[c], kde=True, bins=10, color="steelblue", ax=ax)
    ax.set_title(f"Orgs — {c}")

plt.tight_layout()
plt.show()





'''
excercises_full.csv also has multiple numerical valriables like ExMaturity, ExComplexity, 
ExLength, ExTradeCraftIntra and ExTradeCraftInter. Lets see the numeric distribution of 
these features. This can also determine how the recommender will perform based on distribution of data.
'''

ex_num = ["ExMaturity","ExComplexity","ExLength","ExTradeCraftIntra","ExTradeCraftInter"]

fig, axes = plt.subplots(1, len(ex_num), figsize=(25, 4))  # 1 row, 5 columns

for ax, c in zip(axes, ex_num):
    sns.histplot(df_exs[c], kde=True, bins=10, color="seagreen", ax=ax)
    ax.set_title(f"Exercises — {c}")

plt.tight_layout()
plt.show()






fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

# Orgs correlation heatmap
sns.heatmap(df_orgs[org_num].corr(), annot=True, cmap="coolwarm", ax=axes[0])
axes[0].set_title("orgs_full.csv — Numeric Correlation")

# Exercises correlation heatmap
sns.heatmap(df_exs[ex_num].corr(), annot=True, cmap="coolwarm", ax=axes[1])
axes[1].set_title("exercises_full.csv — Numeric Correlation")

plt.tight_layout()
plt.show()








import re
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def find_col(df: pd.DataFrame, col_like: str) -> str:
    """Find a column by name, case/underscore-insensitive."""
    target = re.sub(r'[^a-z0-9]', '', col_like.lower())
    for c in df.columns:
        cand = re.sub(r'[^a-z0-9]', '', c.lower())
        if cand == target:
            return c
    raise KeyError(f"Column like '{col_like}' not found. Available: {list(df.columns)}")

def normalize_explode(df: pd.DataFrame, col_like: str) -> pd.DataFrame:
    """
    Normalizes separators (; , |), lowercases, underscorizes tokens,
    explodes into tidy rows, and returns a (token, count) DataFrame.
    Works even if your source column is exthreat/ExThreat/ex_threat.
    """
    col = find_col(df, col_like)
    s = df[col].fillna("").astype(str).str.replace("|", ";").str.replace(",", ";")
    exploded = (s.str.split(";")
                  .explode()
                  .str.strip()
                  .str.lower()
                  .str.replace(" ", "_"))
    exploded = exploded[exploded != ""]
    # keep the original 'col' casing for the counts column name
    return exploded.value_counts().reset_index().rename(columns={"index": col, 0: "count"})

def plot_top(df_counts: pd.DataFrame, col_name: str | None = None, topk: int = 10,
             title: str | None = None, color: str = "steelblue", ax=None):
    """
    Plot the top-k counts. If col_name is None, use the first column (token column).
    If ax is provided, plot into that subplot; otherwise create a new figure.
    """
    if col_name is None:
        cols = [c for c in df_counts.columns if c != "count"]
        if not cols:
            raise ValueError("df_counts must contain a token column and 'count'.")
        col_name = cols[0]

    top = df_counts.head(topk)

    if ax is None:
        plt.figure(figsize=(7, 4))
        ax = plt.gca()

    sns.barplot(y=col_name, x="count", data=top, color=color, ax=ax)
    ax.set_title(title or f"Top {topk} {col_name}")
    ax.set_xlabel("Count")
    ax.set_ylabel("")



# Build counts
ex_threat_counts = normalize_explode(df_exs, "ExThreat")
ex_ttp_counts    = normalize_explode(df_exs, "ExTTPs")
ex_group_counts  = normalize_explode(df_exs, "ExGroups")
ex_soft_counts   = normalize_explode(df_exs, "ExSoftware")
ex_cat_counts    = normalize_explode(df_exs, "ExCategories")

# Plot into a grid
fig, axes = plt.subplots(2, 3, figsize=(18,10))

plot_top(ex_threat_counts, topk=10, title="Threats",    color="royalblue", ax=axes[0,0])
plot_top(ex_ttp_counts,    topk=10, title="TTPs",       color="orchid",   ax=axes[0,1])
plot_top(ex_group_counts,  topk=10, title="Groups",     color="teal",     ax=axes[0,2])
plot_top(ex_soft_counts,   topk=10, title="Software",   color="orange",   ax=axes[1,0])
plot_top(ex_cat_counts,    topk=10, title="Categories", color="slateblue",ax=axes[1,1])

axes[1,2].axis("off")  # hide last empty slot

plt.tight_layout()
plt.show()



# Build counts
org_threat_counts   = normalize_explode(df_orgs, "Threats")
org_ttp_counts      = normalize_explode(df_orgs, "TTPs")
org_aim_counts      = normalize_explode(df_orgs, "Aims")

org_industry_counts = df_orgs["Industry"].value_counts().rename_axis("Industry").reset_index(name="count")
org_region_counts   = df_orgs["Region"].value_counts().rename_axis("Region").reset_index(name="count")
org_size_counts     = df_orgs["Size"].value_counts().rename_axis("Size").reset_index(name="count")
org_budget_counts   = df_orgs["SecurityBudget"].value_counts().rename_axis("SecurityBudget").reset_index(name="count")
org_team_counts     = df_orgs["PrimarySecurityTeam"].value_counts().rename_axis("PrimarySecurityTeam").reset_index(name="count")

# Plot into 3x3 grid
fig, axes = plt.subplots(3, 3, figsize=(20,15))
axes = axes.flatten()

plot_top(org_threat_counts,   topk=10, title="Orgs — Top Threats",          color="crimson", ax=axes[0])
plot_top(org_ttp_counts,      topk=10, title="Orgs — Top TTPs",             color="seagreen", ax=axes[1])
plot_top(org_aim_counts,      topk=10, title="Orgs — Top Aims",             color="slateblue", ax=axes[2])
plot_top(org_industry_counts, topk=10, title="Orgs — Industry Distribution",color="royalblue", ax=axes[3])
plot_top(org_region_counts,   topk=10, title="Orgs — Region Distribution",  color="darkorange", ax=axes[4])
plot_top(org_size_counts,     topk=10, title="Orgs — Size Distribution",    color="teal", ax=axes[5])
plot_top(org_budget_counts,   topk=10, title="Orgs — Security Budget",      color="orchid", ax=axes[6])
plot_top(org_team_counts,     topk=10, title="Orgs — Security Team Setup",  color="gray", ax=axes[7])

axes[8].axis("off")  # hide the last unused subplot

plt.tight_layout()
plt.show()






# Combine Threats + TTPs + Aims into one token string per org
def combine_tokens(row):
    parts = []
    for col in ["Threats","TTPs","Aims"]:
        if pd.notna(row[col]):
            parts.append(str(row[col]))
    return ";".join(parts)

df_orgs["all_tokens"] = df_orgs.apply(combine_tokens, axis=1)
df_orgs["all_tokens"].head()


from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer(tokenizer=lambda x: x.split(";"), binary=True)
X = vec.fit_transform(df_orgs["all_tokens"])

print("Matrix shape:", X.shape)  # rows = orgs, cols = unique tokens



from sklearn.decomposition import PCA

pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X.toarray())



from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4, random_state=42)  # choose k=4 (can adjust)
df_orgs["cluster"] = kmeans.fit_predict(X)

df_orgs[["ORGID","cluster"]].head()



import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1],
                hue=df_orgs["cluster"], palette="tab10", s=80)
plt.title("Organisation Clusters based on Threats + TTPs + Aims")
plt.show()



import pandas as pd

for c in sorted(df_orgs["cluster"].unique()):
    subset = df_orgs[df_orgs["cluster"]==c]
    tokens = ";".join(subset["all_tokens"])
    counts = pd.Series(tokens.split(";")).value_counts().head(10)
    print(f"\nCluster {c} (n={len(subset)}) top tokens:")
    print(counts)






from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import hstack, csr_matrix
import re

def split_multi_cell(x: str):
    """Split a multi-valued string cell on ; , | then normalize (lower, underscore)."""
    if pd.isna(x):
        return []
    s = str(x)
    s = s.replace("|", ";").replace(",", ";")
    out = []
    for chunk in s.split(";"):
        t = chunk.strip()
        if t:
            out.append(re.sub(r"\s+", "_", t.lower()))
    return out

def join_prefixed_tokens(row, mapping):
    """
    mapping = [(col_name, prefix), ...]
    For each column in mapping, split and prefix tokens, return a space-joined string.
    """
    toks = []
    for col, pref in mapping:
        vals = split_multi_cell(row.get(col, ""))
        toks.extend([f"{pref}{v}" for v in vals])
    return " ".join(sorted(set(toks)))  # de-dup per row



# ORGS: main matching features + posture
org_cat_map = [
    ("Threats", "threat:"),
    ("TTPs",    "ttp:"),
    ("Aims",    "aim:"),
]
org_num_cols = ["Maturity"]  # you can add ["Complexity","ExerciseFrequency"] later if you want

# EXERCISES: content metadata + numeric descriptors
ex_cat_map = [
    ("ExThreat",     "threat:"),
    ("ExTTPs",       "ttp:"),
    ("ExGroups",     "group:"),
    ("ExSoftware",   "soft:"),
    ("ExCategories", "cat:"),
    ("ExAudience",   "aud:"),
    ("ExStructure",  "struct:"),
]
ex_num_cols = ["ExMaturity","ExComplexity","ExLength","ExTradeCraftIntra","ExTradeCraftInter"]



org_text = df_orgs.apply(lambda r: join_prefixed_tokens(r, org_cat_map), axis=1)
ex_text  = df_exs.apply(lambda r: join_prefixed_tokens(r, ex_cat_map), axis=1)

org_text.head(), ex_text.head()



from sklearn.feature_extraction.text import TfidfVectorizer

# create DTM from categorical tokens
vec = TfidfVectorizer(tokenizer=lambda x: x.split(), binary=False)
X_cat = vec.fit_transform(ex_text)
print("Categorical DTM shape:", X_cat.shape)



# from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

ex_num_cols = ["ExMaturity","ExComplexity","ExLength",
               "ExTradeCraftIntra","ExTradeCraftInter"]

scaler =  MinMaxScaler()
X_num = scaler.fit_transform(df_exs[ex_num_cols])
print("Numeric feature matrix shape:", X_num.shape)



from scipy.sparse import hstack, csr_matrix

# convert numeric to sparse so it stacks cleanly
X_num_sparse = csr_matrix(X_num)

# horizontally stack
X_combined = hstack([X_cat, X_num_sparse])
print("Final feature matrix shape:", X_combined.shape)



from sklearn.metrics.pairwise import cosine_similarity

# pairwise similarities between exercises
sim_matrix = cosine_similarity(X_combined)

# example: top-5 similar exercises to EXID = 0
ex_id = 0
top_idx = np.argsort(sim_matrix[ex_id])[::-1][1:6]  # skip self
print("Top 5 similar exercises:", df_exs.iloc[top_idx]["EXID"].tolist())



import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Assume sim_matrix is (30, 30)
n = sim_matrix.shape[0]
labels = df_exs["EXID"].astype(str).tolist() if "df_exs" in globals() else np.arange(n)

# --- 1. Find top-5 per row (excluding self=1.0) ---
highlight = np.zeros_like(sim_matrix, dtype=bool)
for i in range(n):
    row = sim_matrix[i].copy()
    row[i] = -np.inf            # ignore self-similarity
    top_idx = np.argsort(row)[-5:]  # top 5 indices
    highlight[i, top_idx] = True

# --- 2. Plot heatmap with annotations ---
plt.figure(figsize=(12, 10))
ax = sns.heatmap(sim_matrix,
                 xticklabels=labels,
                 yticklabels=labels,
                 cmap="coolwarm",
                 center=0,
                 annot=True, fmt=".2f",  # show values
                 annot_kws={"size":6},
                 cbar_kws={"label": "Cosine similarity"})

# --- 3. Overlay markers for top-5 similarities ---
for i in range(n):
    for j in range(n):
        if highlight[i, j]:
            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False,
                                       edgecolor="gold", lw=1.5))

plt.title("Exercise–Exercise Similarity (30x30)\nTop-5 per exercise highlighted", fontsize=14)
plt.tight_layout()
plt.show()












from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import chi2_kernel
import numpy as np

# counts from your space-separated, prefixed tokens
vec_count = CountVectorizer(tokenizer=lambda x: x.split())
X_cat_counts = vec_count.fit_transform(ex_text)

# L1-normalize rows → distributions
X_cat_dist = normalize(X_cat_counts, norm="l1")

# chi-square kernel expects dense
X_dense = X_cat_dist.toarray()   # or: np.asarray(X_cat_dist.todense())

chi_gamma = 0.5
sim_chi = chi2_kernel(X_dense, gamma=chi_gamma)  # (n_ex, n_ex), values in (0,1]



import numpy as np

X = X_cat_counts.toarray()
# (optional but recommended) L1-normalize to distributions:
X = X / (X.sum(axis=1, keepdims=True) + 1e-12)

# pairwise chi-square distance
eps = 1e-12
num = (X[:, None, :] - X[None, :, :]) ** 2
den = (X[:, None, :] + X[None, :, :] + eps)
chi2_dist = 0.5 * np.sum(num / den, axis=2)            # (n,n)

# convert distance → similarity
gamma = 0.5
sim_chi = np.exp(-gamma * chi2_dist)                   # (n,n), (0,1]



import numpy as np

X = X_counts if isinstance(X_counts, np.ndarray) else X_counts.toarray()
eps = 1e-12

# Efficient pairwise χ² distance computation
# Expand dims for broadcasting: (n,1,d) vs (1,n,d)
num = (X[:, None, :] - X[None, :, :]) ** 2
den = (X[:, None, :] + X[None, :, :] + eps)
chi2_dist = 0.5 * np.sum(num / den, axis=2)  # shape (n, n)

# Option 1: min-max normalize to [0,1] similarity
mx = chi2_dist.max() if np.isfinite(chi2_dist).any() else 1.0
chi2_sim_mm = 1.0 - (chi2_dist / (mx + eps))

# Option 2: RBF-style similarity (closer to the kernel above)
gamma = 0.5
chi2_sim_rbf = np.exp(-gamma * chi2_dist)

# Use one of the similarities:
chi2_sim = chi2_sim_rbf



import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Assume sim_matrix is (30, 30)
n = sim_chi.shape[0]
labels = df_exs["EXID"].astype(str).tolist() if "df_exs" in globals() else np.arange(n)

# --- 1. Find top-5 per row (excluding self=1.0) ---
highlight = np.zeros_like(sim_chi, dtype=bool)
for i in range(n):
    row = sim_chi[i].copy()
    row[i] = -np.inf            # ignore self-similarity
    top_idx = np.argsort(row)[-5:]  # top 5 indices
    highlight[i, top_idx] = True

# --- 2. Plot heatmap with annotations ---
plt.figure(figsize=(12, 10))
ax = sns.heatmap(sim_chi,
                 xticklabels=labels,
                 yticklabels=labels,
                 cmap="coolwarm",
                 center=0,
                 annot=True, fmt=".2f",  # show values
                 annot_kws={"size":6},
                 cbar_kws={"label": "Chi square similarity"})

# --- 3. Overlay markers for top-5 similarities ---
for i in range(n):
    for j in range(n):
        if highlight[i, j]:
            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False,
                                       edgecolor="gold", lw=1.5))

plt.title("Exercise–Exercise Similarity (30x30)\nTop-5 per exercise highlighted", fontsize=14)
plt.tight_layout()
plt.show()



import numpy as np
import pandas as pd

def topk_neighbors(sim_matrix, df_exs, k=5):
    results = {}
    for i in range(sim_matrix.shape[0]):
        row = sim_matrix[i].copy()
        row[i] = -np.inf  # ignore self
        top_idx = np.argsort(row)[::-1][:k]  # top-k
        results[df_exs.iloc[i]["EXID"]] = df_exs.iloc[top_idx]["EXID"].tolist()
    return results

# Cosine neighbours
cosine_neighbors = topk_neighbors(sim_matrix, df_exs, k=5)

# Chi-square neighbours
chi_neighbors = topk_neighbors(sim_chi, df_exs, k=5)

# Side-by-side comparison
comparison = []
for exid in df_exs["EXID"]:
    comparison.append({
        "EXID": exid,
        "Cosine_Top5": cosine_neighbors[exid],
        "ChiSquare_Top5": chi_neighbors[exid]
    })

df_compare = pd.DataFrame(comparison)



df_compare



