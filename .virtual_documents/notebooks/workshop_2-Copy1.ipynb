





# Importing necesssary libraries
import pandas as pd
import re

# Read the dataset
df_orgs = pd.read_csv("orgs_full.csv")
df_exs  = pd.read_csv("exercises_full.csv")

# Load your mapping CSV (adjust the filename/paths/column names if needed)
df_ttps = pd.read_csv("ENTERPRISE_ATTACK_TECHNIQUES.CSV") 

# Info and data types of the datasets & columns
df_orgs.info(), df_exs.info(), df_ttps.info()






df_orgs.head()


df_exs.head()


df_ttps.head()


# Lambda: normalize one column name
norm_col = lambda c: str(c).strip().lower().replace(" ", "_")
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.rename(columns=norm_col)

# Lets clean the text specially multivalued separated by ;
# splits → trims → removes blanks → sorts → rejoins
def normalize_list_col(s: pd.Series):
    return (s.fillna('')
             .str.split(';')
             .apply(lambda xs: ';'.join(sorted(x.strip() for x in xs if x.strip()))))

# Lets fix the case of the categorical variables
# Convert text to lower case.
def normalise_case(s: pd.Series):
    return (s.astype(str)
         .str.strip()
         .str.replace(r'\s+', ' ', regex=True)
         .str.lower())


# first of all lets normalise the columna names and make it consistent among all dataset.. best practices..
df_orgs = normalize_columns(df_orgs)
df_exs = normalize_columns(df_exs)
df_ttps = normalize_columns(df_ttps)

df_orgs.columns.values, df_exs.columns.values, df_ttps.columns.values


# In this step lets filter out the categorical/text columns which has object dtypes 
cat_cols_orgs = df_orgs.select_dtypes(include='object').columns
cat_cols_exs = df_exs.select_dtypes(include='object').columns
cat_cols_ttps = df_ttps.select_dtypes(include='object').columns

cat_cols_orgs, cat_cols_exs, cat_cols_ttps


# If you look at the features like Threats, Aims, TTPs it is matibvalued separated with semicolon
# Lets normalise it with normalise_list_cols method that -
# (1) replaces any Nan values with empty string '', 
# (2) splits the string by ; to create a list, 
# (3) strips leading and trailing spaces
# (4) sorts aplhabetically and
# (5) joins back into single string.

df_orgs['threats'] = normalize_list_col(df_orgs['threats'])
df_orgs['aims'] = normalize_list_col(df_orgs['aims'])
df_orgs['ttps'] = normalize_list_col(df_orgs['ttps'])

df_exs['exttps']     = normalize_list_col(df_exs['exttps'])
df_exs['exgroups']   = normalize_list_col(df_exs['exgroups'])
df_exs['exsoftware'] = normalize_list_col(df_exs['exsoftware'])
df_exs['excategories'] = normalize_list_col(df_exs['excategories'])


# calling the above normalise_case function to apply for all categorical variables
for col in cat_cols_orgs:
    df_orgs[col] = normalise_case(df_orgs[col])

for col in cat_cols_exs:
        df_exs[col] = normalise_case(df_exs[col])

for col in cat_cols_ttps:
        df_ttps[col] = normalise_case(df_ttps[col])


# Checking if the dataset has any duplicate entries
'''
df_orgs['orgid'].duplicated().any(), df_exs['exid'].duplicated().any()
'''


# There is the creation date field in exercise dataset
# Might use this exercise creation date field so converting it to datetime

'''
df_exs['excreation'] = pd.to_datetime(df_exs['excreation'], errors='coerce')
print(df_exs['excreation'].info())
'''


# Now lets check if there is any missing values.

print(df_orgs.isna().sum())
print()
print(df_exs.isna().sum())
print()
print(df_ttps.isna().sum())


# Lets see the unique values preent in each categorical variable

orgs_unique_val = df_orgs[cat_cols_orgs].nunique().sort_values(ascending=False)
exe_unique_val  = df_exs[cat_cols_exs].nunique().sort_values(ascending=False)

print(orgs_unique_val)
print()
print(exe_unique_val)


df_ttps.head()


df_ttps['name_norm'] = (df_ttps['name']
                        .str.strip()
                        .str.lower()
                        .str.replace(r'\s+', ' ', regex=True))


df_ttps.head()


# Build fast lookups
NAME2ID = dict(zip(df_ttps['name'], df_ttps['id']))


NAME2ID


# EDA to know the dateset better

# cyber posture vs maturity
#exground is mitre attack catetogy
#maturity links to impact in CVEs

#maturity in excercise datsaet depesnds on TTPs and frewuency and organisatio is different
#cluster organisation based on threat groups
#develop a metrics to measure sucess 
#extradecraft similarity score inside and outside ..

from collections import Counter

def explode_counts(df, col):
    counts = Counter()
    df[col].dropna().apply(lambda x: counts.update([t.strip() for t in str(x).split(";")]))
    return pd.DataFrame(counts.most_common(), columns=[col, "count"])

print(explode_counts(df_exs, "exthreat").head(10))
print(explode_counts(df_exs, "exttps").head(10))

print(explode_counts(df_orgs, "threats").head(10))
print(explode_counts(df_orgs, "ttps").head(10))




from sklearn.preprocessing import MultiLabelBinarizer

def onehot_multivalued(series):
    series = series.fillna("").apply(lambda x: [s.strip() for s in x.split(";") if s.strip()])
    mlb = MultiLabelBinarizer()
    return pd.DataFrame(mlb.fit_transform(series), columns=mlb.classes_, index=series.index)

ex_threat_ohe = onehot_multivalued(df_exs["exthreat"])
print(ex_threat_ohe.head())


for c in ["exthreat","exttps","excategories","exaudience","exstructure","exgroups"]:
    uniques = explode_counts(df_exs, c)
    print(f"{c}: {len(uniques)} unique, top5 = {uniques.head(5).to_dict('records')}")



df_exs[["exmaturity","excomplexity","exlength","extradecraftintra","extradecraftinter"]].hist(figsize=(10,6))



def merge_cols(row, cols):
    toks = []
    for c in cols:
        if pd.isna(row.get(c, "")): continue
        vals = [t.strip() for t in str(row[c]).split(";") if t.strip()]
        toks.extend([c.lower()+"_"+t.lower().replace(" ","_") for t in vals])
    return " ".join(toks)

df_exs["threat_doc"] = df_exs.apply(
    lambda r: merge_cols(r, ["exthreat","exgroups"]), axis=1
)

print(df_exs[["exid","exthreat","exgroups","threat_doc"]].head(5))





def split_semicolons(s):
    if pd.isna(s) or not str(s).strip(): return []
    return [p.strip() for p in str(s).split(';') if p.strip()]

def norm_token(x):  # "Banking Trojan" -> "banking_trojan"
    return re.sub(r'\s+', '_', str(x).strip().lower())

def threat_tokens(s):
    return [f"threat_{norm_token(x)}" for x in split_semicolons(s)]

def ttp_tokens(s):
    return [f"ttp_{norm_token(x)}" for x in split_semicolons(s)]



# orgs: threats + ttps
org_docs = (
    df_orgs
    .assign(_tok=lambda d: d['threats'].map(threat_tokens) + d['ttps'].map(ttp_tokens))
    .assign(doc=lambda d: d['_tok'].apply(lambda xs: ' '.join(sorted(set(xs)))))
    [['orgid','doc']]
    .set_index('orgid')['doc']
)

# exercises: exthreat + exttps
ex_docs = (
    df_exs
    .assign(_tok=lambda d: d['exthreat'].map(threat_tokens) + d['exttps'].map(ttp_tokens))
    .assign(doc=lambda d: d['_tok'].apply(lambda xs: ' '.join(sorted(set(xs)))))
    [['exid','doc']]
    .set_index('exid')['doc']
)



org_docs.values


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

corpus = list(org_docs.values) + list(ex_docs.values)
vec = TfidfVectorizer(lowercase=True, ngram_range=(1,1), min_df=1,  token_pattern=r"[^ ]+")
vec.fit(corpus)

org_X = vec.transform(org_docs.values)
ex_X  = vec.transform(ex_docs.values)

# feat = vec.get_feature_names_out()
# scale = np.ones_like(feat, dtype=float)
# scale[[f.startswith("ttp_") for f in feat]] = 2.0   # try 2–4×

# org_Xw = org_X @ np.diag(scale)
# ex_Xw  = ex_X  @ np.diag(scale)

sim_df = pd.DataFrame(cosine_similarity(org_X, ex_X),
                      index=org_docs.index, columns=ex_docs.index)


def recommend_for_org(org_id, top_n=5):
    s = sim_df.loc[org_id].sort_values(ascending=False).head(top_n)
    df = s.reset_index()
    df.columns = ['exercise', 'score']   # force names
    df['score'] = df['score'].round(4)
    return df


# Top 5 exercises recommended for Org with ID=1
print(recommend_for_org(1, top_n=5))


sim_df


# lets check if this recommendaer mataches with training dataset

df_training = pd.read_csv("ratings_train_full.csv")
df_training = normalize_columns(df_training)
df_training.info()


df_training[df_training['orgid'] == 1]


df_exs["ExTTPs"]


def normalize_list_col(s: pd.Series):
    return (s.fillna('')
             .str.split(';')
             .apply(lambda xs: ' '.join(sorted(x.strip() for x in xs if x.strip()))))


normalize_list_col(df_exs["ExTTPs"])


def split_semicolons(s):
    if pd.isna(s) or not str(s).strip(): return []
    return [p.strip() for p in str(s).split(';') if p.strip()]

def norm_token(x):  # "Banking Trojan" -> "banking_trojan"
    return re.sub(r'\s+', '_', str(x).strip().lower())

def threat_tokens(s):
    return [f"threat_{norm_token(x)}" for x in split_semicolons(s)]

def ttp_tokens(s):
    return [f"ttp_{norm_token(x)}" for x in split_semicolons(s)]


org_docs = (
    df_orgs
    .assign(_tok=lambda d: d['Threats'].map(threat_tokens) + d['TTPs'].map(ttp_tokens))
    .assign(doc=lambda d: d['_tok'].apply(lambda xs: ' '.join(sorted(set(xs)))))
    [['ORGID','doc']]
    .set_index('ORGID')['doc']
)


org_docs.values






