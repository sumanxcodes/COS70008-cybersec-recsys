# Importing necesssary libraries
import pandas as pd
import re

# Read the dataset
df_orgs = pd.read_csv("orgs_full.csv")
df_exs  = pd.read_csv("exercises_full.csv")

# Info and data types of the datasets & columns
df_orgs.info(), df_exs.info(), df_ttps.info()



# Lambda: normalize one column name
norm_col = lambda c: str(c).strip().lower().replace(" ", "_")
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.rename(columns=norm_col)

# Lets clean the text specially multivalued separated by ;
# splits → trims → removes blanks → sorts → rejoins
def normalize_list_col(s: pd.Series):
    return (s.fillna('')
             .str.split(';')
             .apply(lambda xs: ';'.join(sorted(x.strip() for x in xs if x.strip()))))

# Lets fix the case of the categorical variables
# Convert text to lower case.
def normalise_case(s: pd.Series):
    return (s.astype(str)
         .str.strip()
         .str.replace(r'\s+', ' ', regex=True)
         .str.lower())


# first of all lets normalise the columna names and make it consistent among all dataset.. best practices..
df_orgs = normalize_columns(df_orgs)
df_exs = normalize_columns(df_exs)


df_orgs.columns.values, df_exs.columns.values, df_ttps.columns.values


# ---------- helpers fucntion to clean dataset ----------

def norm_col(c: str) -> str:
    return str(c).strip().lower().replace(" ", "_")

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.rename(columns=norm_col)

def split_semicolons(val):
    if pd.isna(val) or not str(val).strip():
        return []
    return [p.strip() for p in str(val).split(";") if p.strip()]

def norm_token(s: str) -> str:
    s = str(s).strip().lower()
    s = " ".join(s.split())
    return s.replace(" ", "_")

def bucket_numeric(v, bins, labels):
    if pd.isna(v):
        return "unknown"
    idx = np.digitize([float(v)], bins=bins, right=True)[0]
    idx = min(idx, len(labels)-1)
    return labels[idx]



# Lets concentrate on the exercise dataset only for now..

# Expected exercise feature/columns.. after normalisation
EXPECTED_EX = [
    "exid","exthreat","exttps","excategories","exgroups","exsoftware",
    "exstructure","exaudience","exmaturity","excomplexity","exlength",
    "extradecraftintra","extradecraftinter"
]
missing = [c for c in EXPECTED_EX if c not in df_exs.columns]
if missing: print("misssing exercise cols:", missing)

def tokens_for_exercise(row):
    toks = []
    # Multi-valued categorical → prefixed tokens
    for field, prefix in [
        ("exthreat",    "threat_"),
        ("exttps",      "ttp_"),
        ("excategories","cat_"),
        ("exgroups",    "group_"),
        ("exsoftware",  "soft_"),
        ("exstructure", "struct_"),
        ("exaudience",  "aud_"),
    ]:
        for val in split_semicolons(row.get(field, "")):
            toks.append(prefix + norm_token(val))

    # Numeric → buckets → tokens
    mat = bucket_numeric(row.get("exmaturity", np.nan),   bins=[2,3,5],   labels=["L","M","H"])
    cpx = bucket_numeric(row.get("excomplexity", np.nan), bins=[2,3,5],   labels=["L","M","H"])
    dur = bucket_numeric(row.get("exlength", np.nan),     bins=[60,120,1e9], labels=["short","medium","long"])
    toks += [f"maturity_{mat}", f"complexity_{cpx}", f"len_{dur}"]

    # Tradecraft scores (0..1) → coarse buckets
    for field, prefix in [("extradecraftintra","td_intra_"), ("extradecraftinter","td_inter_")]:
        v = row.get(field, np.nan)
        if pd.isna(v): bucket = "unknown"
        elif float(v) <= 1/3: bucket = "L"
        elif float(v) <= 2/3: bucket = "M"
        else: bucket = "H"
        toks.append(prefix + bucket)

    return " ".join(sorted(set(toks)))

ex_docs = (
    df_exs.assign(doc=lambda d: d.apply(tokens_for_exercise, axis=1))
       [["exid","doc"]]
       .dropna(subset=["exid"])
       .astype({"exid": int})
       .set_index("exid")["doc"]
)

print("Built exercise docs:", ex_docs.shape)
pd.set_option("display.max_colwidth", None)
print(ex_docs.head(1).to_string())




# lets chceck the document we created... 
ex_docs.values


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# We already space-join tokens; keep case as-is & treat any non-space as a token char.
tfidf = TfidfVectorizer(lowercase=False, token_pattern=r"[^ ]+")
X_ex  = tfidf.fit_transform(ex_docs.values)   # rows align to ex_docs.index order

print("DTM shape (exercises × features):", X_ex.shape)

ex_sim = cosine_similarity(X_ex, X_ex)
ex_sim_df = pd.DataFrame(ex_sim, index=ex_docs.index, columns=ex_docs.index)
print("Similarity matrix shape:", ex_sim_df.shape)




ex_sim_df


def similar_exercises(target_exid: int, top_n: int = 5) -> pd.DataFrame:
    if target_exid not in ex_sim_df.index:
        raise KeyError(f"exid {target_exid} not found")
    s = ex_sim_df.loc[target_exid].drop(index=target_exid)
    s = s.sort_values(ascending=False).head(top_n).round(4)
    out = s.reset_index()
    out.columns = ["exid","similarity"]
    return out

print(similar_exercises(1, 5))
print(similar_exercises(10, 5))



def tokens_for_org(row):
    toks = []
    for field, prefix in [("threats","threat_"), ("ttps","ttp_"), ("aims","aim_")]:
        for val in split_semicolons(row.get(field, "")):
            toks.append(prefix + norm_token(val))
    # Bucket maturity/complexity to match exercise tokens
    mat = bucket_numeric(row.get("maturity", np.nan),   bins=[2,3,5], labels=["L","M","H"])
    cpx = bucket_numeric(row.get("complexity", np.nan), bins=[2,3,5], labels=["L","M","H"])
    toks += [f"maturity_{mat}", f"complexity_{cpx}"]
    # Optional org size
    if "size" in row and isinstance(row["size"], str) and row["size"].strip():
        toks.append("size_" + norm_token(row["size"]))
    return " ".join(sorted(set(toks)))

org_docs = (
    df_orgs.assign(doc=lambda d: d.apply(tokens_for_org, axis=1))
        [["orgid","doc"]]
        .dropna(subset=["orgid"])
        .astype({"orgid": int})
        .set_index("orgid")["doc"]
)

X_org = tfidf.transform(org_docs.values)      # same vocabulary
org_ex_sim = cosine_similarity(X_org, X_ex)
org_ex_sim_df = pd.DataFrame(org_ex_sim, index=org_docs.index, columns=ex_docs.index)

def recommend_for_org(org_id: int, top_n: int = 5) -> pd.DataFrame:
    if org_id not in org_ex_sim_df.index:
        raise KeyError(f"orgid {org_id} not found")
    s = org_ex_sim_df.loc[org_id].sort_values(ascending=False).head(top_n).round(4)
    out = s.reset_index()
    out.columns = ["exid","similarity"]
    return out

# EXAMPLE
print(recommend_for_org(1, 5))



def reweight_by_prefix(X, vocab: dict, prefix: str, factor: float):
    idxs = [i for tok, i in vocab.items() if tok.startswith(prefix)]
    if not idxs: return X
    X = X.tocsc(copy=True)
    X[:, idxs] = X[:, idxs] * factor
    return X.tocsr()

# Rebuild a weighted similarity using TTP weight = 3.0
vocab = tfidf.vocabulary_
X_ex_w = reweight_by_prefix(X_ex, vocab, "ttp_", 2.0)
ex_sim_w = cosine_similarity(X_ex_w, X_ex_w)
ex_sim_w_df = pd.DataFrame(ex_sim_w, index=ex_docs.index, columns=ex_docs.index)

def most_similar_exercises_weighted(target_exid: int, top_n: int = 5) -> pd.DataFrame:
    if target_exid not in ex_sim_w_df.index:
        raise KeyError(f"exid {target_exid} not found")
    s = ex_sim_w_df.loc[target_exid].drop(index=target_exid)
    s = s.sort_values(ascending=False).head(top_n).round(4)
    out = s.reset_index()
    out.columns = ["exid","similarity"]
    return out

# EXAMPLE
print(most_similar_exercises_weighted(1, 5))



org_ex_sim_df



